# Tests AgentForge

Suite complÃ¨te de tests pour valider le systÃ¨me AgentForge.

## ğŸ¯ Vue d'Ensemble

Ce dossier contient 5 types de tests pour garantir la qualitÃ© et robustesse d'AgentForge :

1. **Tests Complets** - ScÃ©narios rÃ©els de gÃ©nÃ©ration
2. **Benchmarks** - Performance et mÃ©triques
3. **Tests de Stress** - Robustesse et edge cases
4. **Tests UI** - Interface Flask
5. **Suite MaÃ®tre** - Orchestration de tous les tests

## ğŸš€ ExÃ©cution Rapide

### Windows (PowerShell)
```powershell
.\scripts\run_tests.ps1
```

### Linux/Mac
```bash
python scripts/run_all_tests.py
```

## ğŸ“‹ Scripts Disponibles

### 1. `test_comprehensive.py`
- **But**: Tests de scÃ©narios rÃ©els complets
- **ScÃ©narios**: 6 cas d'usage (simple, e-commerce, blog, etc.)
- **Validation**: Pipeline complet avec Ã©valuation
- **DurÃ©e**: ~2-3 minutes

```python
python scripts/test_comprehensive.py
```

### 2. `benchmark.py`
- **But**: Mesure de performances
- **MÃ©triques**: Temps, mÃ©moire, gÃ©nÃ©ration de fichiers
- **Comparaison**: Manual vs LangGraph
- **DurÃ©e**: ~5-10 minutes

```python
python scripts/benchmark.py
```

### 3. `stress_test.py`
- **But**: Tests de robustesse
- **Edge Cases**: Prompts vides, syntaxe cassÃ©e, entitÃ©s multiples
- **Validation**: Gestion gracieuse des erreurs
- **DurÃ©e**: ~3-5 minutes

```python
python scripts/stress_test.py
```

### 4. `ui_integration_test.py`
- **But**: Tests de l'interface Flask
- **Tests**: Endpoints, gÃ©nÃ©ration via UI, API
- **Validation**: Interface utilisateur complÃ¨te
- **DurÃ©e**: ~2-4 minutes
- **Note**: DÃ©marre automatiquement le serveur Flask

```python
python scripts/ui_integration_test.py
```

### 5. `run_all_tests.py`
- **But**: Suite complÃ¨te orchestrÃ©e
- **ExÃ©cution**: Tous les tests sÃ©quentiellement
- **Rapport**: ConsolidÃ© avec mÃ©triques globales
- **DurÃ©e**: ~10-20 minutes

```python
python scripts/run_all_tests.py
```

## ğŸ“Š Fichiers de RÃ©sultats

AprÃ¨s exÃ©cution, les rÃ©sultats sont sauvegardÃ©s en JSON :

- `test_comprehensive_results.json` - RÃ©sultats tests complets
- `benchmark_results.json` - MÃ©triques de performance
- `stress_test_results.json` - RÃ©sultats robustesse
- `ui_integration_test_results.json` - RÃ©sultats interface
- `master_test_report.json` - Rapport consolidÃ© global

## ğŸ¯ CritÃ¨res de RÃ©ussite

### Tests Complets
- âœ… Score â‰¥ 0.7 pour chaque scÃ©nario
- âœ… 100% des fichiers gÃ©nÃ©rÃ©s
- âœ… Pipeline complet sans erreur

### Benchmarks
- âœ… GÃ©nÃ©ration < 30s par projet
- âœ… MÃ©moire < 500MB
- âœ… Consistance entre modes

### Tests de Stress
- âœ… 0 crash (gestion gracieuse)
- âœ… Taux de robustesse â‰¥ 90%
- âœ… Edge cases gÃ©rÃ©s

### Tests UI
- âœ… Tous endpoints fonctionnels
- âœ… GÃ©nÃ©ration via interface
- âœ… Temps de rÃ©ponse < 5s

### Suite Globale
- âœ… Toutes suites rÃ©ussies
- âœ… Taux global â‰¥ 85%
- âœ… Rapport consolidÃ© gÃ©nÃ©rÃ©

## ğŸ”§ Configuration

### PrÃ©requis
```bash
pip install -r requirements.txt
```

### Variables d'Environnement (optionnel)
```bash
export AGENTFORGE_TEST_TIMEOUT=300  # Timeout par dÃ©faut
export AGENTFORGE_UI_PORT=5000      # Port Flask pour tests UI
```

### Configuration LLM
Les tests utilisent la configuration LLM du projet :
- OpenAI API (si configurÃ©e)
- Fallback vers gÃ©nÃ©ration dÃ©terministe

## ğŸ“ˆ InterprÃ©tation des RÃ©sultats

### Scores d'Ã‰valuation
- **1.0** : Parfait (tous fichiers + structure correcte)
- **0.8-0.9** : Excellent (quelques dÃ©tails mineurs)
- **0.6-0.7** : Bon (fonctionnel avec amÃ©liorations)
- **0.4-0.5** : Acceptable (basique mais utilisable)
- **< 0.4** : Insuffisant (corrections nÃ©cessaires)

### Temps de Performance
- **< 10s** : Excellent
- **10-20s** : Bon
- **20-30s** : Acceptable
- **> 30s** : Ã€ optimiser

### Robustesse
- **0 crash** : Excellent
- **1-2 crashes** : Acceptable
- **> 3 crashes** : Ã€ corriger

## ğŸ› DÃ©pannage

### ProblÃ¨mes Courants

1. **TimeoutError**
   - Augmenter le timeout
   - VÃ©rifier la configuration LLM

2. **ModuleNotFoundError**
   ```bash
   pip install -r requirements.txt
   ```

3. **Port 5000 occupÃ© (tests UI)**
   - Changer le port ou arrÃªter le service existant
   - Le script dÃ©tecte automatiquement les conflits

4. **Permissions d'Ã©criture**
   - VÃ©rifier les droits sur le dossier
   - Les tests crÃ©ent des fichiers temporaires

### Logs de Debug
Activer les logs dÃ©taillÃ©s :
```bash
export AGENTFORGE_DEBUG=1
python scripts/run_all_tests.py
```

## ğŸ“ Usage pour PrÃ©sentation

### Pour l'Oral
1. ExÃ©cuter la suite complÃ¨te une fois : `run_all_tests.py`
2. Montrer le rapport consolidÃ© : `master_test_report.json`
3. DÃ©tailler les mÃ©triques clÃ©s :
   - Taux de rÃ©ussite global
   - Temps de gÃ©nÃ©ration moyens
   - Robustesse (0 crash)
   - Couverture fonctionnelle

### DÃ©monstration Live
1. Test rapide : `test_comprehensive.py`
2. Montrer un scÃ©nario spÃ©cifique
3. Expliquer les scores d'Ã©valuation
4. PrÃ©senter la gÃ©nÃ©ration de fichiers

## ğŸ“š Architecture des Tests

```
scripts/
â”œâ”€â”€ test_comprehensive.py    # Tests scÃ©narios complets
â”œâ”€â”€ benchmark.py            # MÃ©triques performance  
â”œâ”€â”€ stress_test.py          # Robustesse et edge cases
â”œâ”€â”€ ui_integration_test.py  # Interface Flask
â”œâ”€â”€ run_all_tests.py        # Orchestrateur principal
â””â”€â”€ run_tests.ps1          # Script PowerShell Windows
```

Chaque script est autonome mais s'intÃ¨gre dans la suite globale pour une validation complÃ¨te du systÃ¨me AgentForge.

#!/usr/bin/env python3
"""
Diagnostic LLM AgentForge
V√©rifie pourquoi le LLM passe en fallback
"""

import os
import sys
from pathlib import Path

# Add project to path
ROOT = Path(__file__).resolve().parents[1]
sys.path.append(str(ROOT))

from core.llm_client import LLMClient

def test_llm_detailed():
    """Test d√©taill√© de la connexion LLM avec logs"""
    print("üîç DIAGNOSTIC LLM D√âTAILL√â")
    print("=" * 50)
    
    # Test avec configuration Ollama
    os.environ["AGENTFORGE_LLM"] = "ollama"
    os.environ["OLLAMA_MODEL"] = "llama3.1:latest"
    
    client = LLMClient()
    print(f"Provider configur√©: {client.provider}")
    
    # Test 1: Connexion basique
    print(f"\nüì° TEST 1: Connexion basique")
    system_prompt = "R√©ponds en JSON avec {\"status\": \"ok\", \"message\": \"test r√©ussi\"}"
    user_prompt = "Test de connexion"
    
    try:
        print(f"Appel LLM en cours...")
        import time
        start_time = time.time()
        
        result = client.extract_json(system_prompt, user_prompt)
        
        duration = time.time() - start_time
        print(f"‚è±Ô∏è Dur√©e: {duration:.2f}s")
        
        if result is None:
            print(f"‚ùå R√©sultat: None (fallback activ√©)")
            return False
        else:
            print(f"‚úÖ R√©sultat: {result}")
            return True
            
    except Exception as e:
        print(f"üí• Erreur: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_spec_extraction():
    """Test l'extraction de spec comme dans le vrai pipeline"""
    print(f"\nüß™ TEST 2: Extraction de sp√©cification")
    
    system_prompt = (
        "Tu extrais des sp√©cifications d'un prompt utilisateur. R√©ponds UNIQUEMENT en JSON valide avec la structure:"
        '{"entities": [{"name": "EntityName", "fields": ["field1", "field2"]}], "tech": {"web": "fastapi", "db": "postgres"}, "features": []}'
    )
    
    user_prompt = "API avec users(email unique, password_hash) et products(name, price float)"
    
    client = LLMClient()
    
    try:
        print(f"Extraction en cours...")
        import time
        start_time = time.time()
        
        result = client.extract_json(system_prompt, user_prompt)
        
        duration = time.time() - start_time
        print(f"‚è±Ô∏è Dur√©e: {duration:.2f}s")
        
        if result is None:
            print(f"‚ùå Extraction √©chou√©e ‚Üí Fallback d√©terministe")
            return False
        else:
            print(f"‚úÖ Extraction r√©ussie:")
            import json
            print(json.dumps(result, indent=2, ensure_ascii=False))
            return True
            
    except Exception as e:
        print(f"üí• Erreur extraction: {e}")
        return False

def test_ollama_direct():
    """Test direct d'Ollama sans passer par LLMClient"""
    print(f"\nüîß TEST 3: Ollama direct")
    
    try:
        import requests
        import json
        
        # Test API tags
        print("Test /api/tags...")
        response = requests.get("http://localhost:11434/api/tags", timeout=5)
        if response.status_code == 200:
            tags_data = response.json()
            models = tags_data.get("models", [])
            print(f"‚úÖ {len(models)} mod√®les disponibles:")
            for model in models[:3]:
                print(f"  - {model.get('name', 'unknown')}")
        else:
            print(f"‚ùå /api/tags failed: {response.status_code}")
            return False
        
        # Test g√©n√©ration
        print(f"\nTest g√©n√©ration avec llama3.1:latest...")
        payload = {
            "model": "llama3.1:latest",
            "prompt": "R√©ponds juste 'Hello' en JSON: {\"message\": \"Hello\"}",
            "format": "json",
            "stream": False,
        }
        
        import time
        start_time = time.time()
        response = requests.post("http://localhost:11434/api/generate", json=payload, timeout=30)
        duration = time.time() - start_time
        
        if response.status_code == 200:
            data = response.json()
            ollama_response = data.get("response", "")
            print(f"‚úÖ R√©ponse Ollama ({duration:.2f}s): {ollama_response}")
            
            # Try to parse as JSON
            try:
                json_result = json.loads(ollama_response)
                print(f"‚úÖ JSON valide: {json_result}")
                return True
            except:
                print(f"‚ö†Ô∏è R√©ponse non-JSON: {ollama_response}")
                return False
        else:
            print(f"‚ùå G√©n√©ration failed: {response.status_code}")
            return False
            
    except Exception as e:
        print(f"üí• Erreur Ollama direct: {e}")
        return False

def main():
    print("üîç DIAGNOSTIC POURQUOI FALLBACK ?")
    print("V√©rifie pourquoi Ollama passe en mode d√©terministe")
    print("=" * 60)
    
    # Tests s√©quentiels
    test1_ok = test_llm_detailed()
    test2_ok = test_spec_extraction()
    test3_ok = test_ollama_direct()
    
    print(f"\n" + "=" * 60)
    print(f"üìä R√âSULTATS DIAGNOSTIC")
    print(f"=" * 60)
    
    print(f"Test connexion basique: {'‚úÖ OK' if test1_ok else '‚ùå √âCHEC'}")
    print(f"Test extraction spec: {'‚úÖ OK' if test2_ok else '‚ùå √âCHEC'}")
    print(f"Test Ollama direct: {'‚úÖ OK' if test3_ok else '‚ùå √âCHEC'}")
    
    if not any([test1_ok, test2_ok, test3_ok]):
        print(f"\nüö® DIAGNOSTIC: Ollama compl√®tement inaccessible")
        print(f"   ‚Üí V√©rifiez qu'Ollama tourne")
        print(f"   ‚Üí Commande: ollama serve")
    elif test3_ok and not test1_ok:
        print(f"\nüîß DIAGNOSTIC: Probl√®me dans LLMClient")
        print(f"   ‚Üí Ollama fonctionne mais LLMClient √©choue")
        print(f"   ‚Üí Probl√®me de timeout, parsing JSON ou exception")
    elif test1_ok and not test2_ok:
        print(f"\nüìù DIAGNOSTIC: Probl√®me de prompt complexe")
        print(f"   ‚Üí Connexion OK mais extraction de spec √©choue")
        print(f"   ‚Üí Le mod√®le n'arrive pas √† respecter le format JSON demand√©")
    else:
        print(f"\n‚úÖ DIAGNOSTIC: LLM devrait fonctionner")
        print(f"   ‚Üí Probl√®me peut-√™tre dans l'interface Flask")
        print(f"   ‚Üí Ou dans la gestion des variables d'environnement")
    
    print(f"\nüí° POUR FORCER OLLAMA:")
    print(f"   $env:AGENTFORGE_LLM=\"ollama\"")
    print(f"   $env:OLLAMA_MODEL=\"llama3.1:latest\"")
    print(f"   Puis relancer Flask")

if __name__ == "__main__":
    main()
